---
title: "PRC Methods Presentation - March 12, 2019"
output:
  word_document: default
  html_notebook: default
  pdf_document: default
  html_document:
    df_print: paged
---
 - Note: This script is modeled off of an example created by Joshua Rosenberg, Ran Xu, and Ken Frank dated January 20, 2019.
 
In social research, we rarely have access to complete populations. Instead, we rely on samples to generate estimates and draw inferences regarding the entire population. Because we are typically always working with samples, how confident can we be that the our inferences aren't the result of sampling bias? If the phenomenon we are interested in present within our sample, how confident are we that we didn't just luck out by selecting the cases where this happens to be true? If our sample was slightly diffferent, could this dramatically change our conclusions?

In other words, how much bias must be present in our sample in order to invalidate an inference? This script walks through a new technique developed by Kenneth Frank and colleagues (2013) to provide insights onto this question.

Using Ruben's causal model, this technique calculates the percent of cases that would have to be null cases in order for a significant association to go away. Similar to p-values, there is not set threshold that the results need to cross to demonstrate that bias is not an issue. Instead, it is up to researchers to interpret how much bias they are willing to be okay with -- but higher the value the less researchers should be concerned with sampling bias.


```{r setup}
knitr::opts_knit$set(root.dir = '/Users/krisvelasco/Documents/UT-Austin/Admin/PRC Consultants/March 12 Presentation')
```


```{r}

# Clear the workspace and do a gabarbage collection
rm(list=ls())
gc()


# Importing a Stata dataset
library(haven)

lgbti <- read_dta("/Users/krisvelasco/Documents/UT-Austin/Dissertation/Datasets/Master/dissertation_2018only.dta")
```

For this example, I will be doing a simple linear regression model (but the tools presented are also available for other types of models like general linear models, mixed models, etc.)

Let's say I am wanting evaluate how ties to transnational advocacy networks predict LGBTI policies, while controlling for % of women in parliament, % internet users, logged population density, trade as % of GDP, measurement of liberal democracy, and logged GDP per capita (note: all variables are lagged one year).

- FYI: lgbti_eigen = eigenvector centrality within transnational LGBTI advocacy networks



```{r}
model1 <- lm(progressive_index ~ lgbti_eigen + 
               lag_women_parliament + 
               lag_internet_users + 
               lag_population_density_ln +
               lag_trade  +
               lag_libdem_vdem +
               lag_gdppercap_ln, 
                data = lgbti)

# Now let's look at the output from this model
summary(model1)
```

### Question: How many countries would have to have no association between network centrality and progressive policies to invalidate this inference?

In other words, how biased would our sample have to be in order to invalidate this significant association? To answer that question, we are going to use the new Konfound tool developed by Frank and colleagues.

```{r}
library(konfound)
```

We are now going to test the bias in our regression model using the following command:
konfound(model1, lgbti_eigen)

- model1 is the object holding the results from our linear regression
- lgbti_eigen is the predictor we are trying to invalidate

```{r}
konfound(model1, lgbti_eigen)
```

### 59.72% of the estimate would have to be due to bias to invalidate the inference

From the output, we can see that in order to invalidate the inference, 59.72% of the estimate would have to be due to bias -- or 91 observations would have to be replaced with cases in which the effect of centrality on progressive policies is 0.

Further, the output tells us the extent to which a confounding variable could nullify our results. In this example, an omitted variable would have to be correlated at .508 with the outcome and predictor variable to invalidate the inference.

## Visualizing Output
```{r}
konfound_output <- konfound(model1, lgbti_eigen, to_return = c("raw_output", "thresh_plot", "corr_plot"))
konfound_output$thresh_plot
```

The region above the threshold represents the percent of the estimated coefficient that would have to be replaced with null cases in order to invalidate the significant effect. 

The next plot showcases how much an omitted variable would have to correlate with both our predictor of interest and the outcome variable in order to invalidate the inference.

```{r}
konfound_output$corr_plot
```


### How to learn more about sensitivity analysis

To learn more about sensitivity analysis, please visit:

* The [Introduction to konfound vignette](https://jrosen48.github.io/konfound/articles/Introduction_to_konfound.html), with detailed information about each of the functions (`pkonfound()`, `konfound()`, and `mkounfound()`)
* The causal inference section of Ken Frank's website [here](https://msu.edu/~kenfrank/research.htm#causal)
* The [konfound interactive web application](https://jmichaelrosenberg.shinyapps.io/shinykonfound/), with links to PowerPoints and key publications

### References

* Frank, K.A., Maroulis, S., Duong, M., and Kelcey, B. 2013. What would it take to change an inference?: Using Rubinâ€™s causal model to interpret the robustness of causal inferences. *Education, Evaluation and Policy Analysis*. Vol 35: 437-460. https://msu.edu/~kenfrank/What%20would%20it%20take%20to%20Change%20an%20Inference%20published.docx

* Frank, K.A., Gary Sykes, Dorothea Anagnostopoulos, Marisa Cannata, Linda Chard, Ann Krause, Raven McCrory. 2008. Extended influence: National Board Certified Teachers as help providers. *Education, Evaluation, and Policy Analysis*. Vol 30(1): 3-30. https://msu.edu/~kenfrank/papers/Does%20NBPTS%20Certification%20Affect%20the%20Number%20of%20Colleagues%20a%20Teacher%20Helps%20with%20Instructional%20Matters%20acceptance%20version%202.doc

* Frank, K. A. and Min, K. 2007. Indices of Robustness for Sample Representation. *Sociological Methodology*. Vol 37, 349-392. https://msu.edu/~kenfrank/papers/INDICES%20OF%20ROBUSTNESS%20TO%20CONCERNS%20REGARDING%20THE%20REPRESENTATIVENESS%20OF%20A%20SAMPLE.doc (co first authors)

* Frank, K. 2000. "Impact of a Confounding Variable on the Inference of a Regression Coefficient." *Sociological Methods and Research*, 29(2), 147-194 https://msu.edu/~kenfrank/papers/impact%20of%20a%20confounding%20variable.pdf